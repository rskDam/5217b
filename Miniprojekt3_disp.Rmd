---
title: "Miniprojekt3_disp"
author: "Gruppe 5.217"
date: "15/1/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(optimsimplex)
library(microbenchmark)
library(numDeriv)
```

## Secant equations

\begin{itemize}
  \item $f(x_k+p)= f(x_k)+\nabla f(x_k)^Tp+\frac{1}{2}p^T \nabla^2 f(x_k)p=m_k(p)$
  \item $m_{k+1}(p)=f_{k+1}+\nabla f_{k+1}^Tp+\frac{1}{2}p^T B_{k+1} p$
  \item $\nabla m_{k+1}(p)= \nabla f_{k+1}^T+B_{k+1}p$
  \item $\nabla m_{k+1}(0)= \nabla f_{k+1}$
  \item $\nabla m_{k+1}( -\alpha p_k)=\nabla f_{k+1} -\alpha B_{k+1}p_k=\nabla f_{k}$
  \item $x_{k+1}=x_k+s_k$
  \item $B_{k+1}s_k=\nabla f_{k+1}-\nabla f_k$
  \item $y_k=\nabla f_{k+1}-\nabla f_k$
  \item $B_{k+1}s_k=y_k$
\end{itemize}

## BFGS

\begin{itemize}
  \item Hessian udregnes ikke, men approximeres.
  \item $H_{k+1}y_k=s_k$
  \item $H_{k+1}=(I-\rho _ks_ky_k^T)H_k(I-\rho_ky_ks_k^T)+\rho _ks_ks_k^T$
  \item $\rho_k=\frac{1}{y_k^Ts_k}$
  \item $x_{k+1}=x_k-\alpha_k H_k \nabla f_k$
  \item Hvordan vælges $H_0$
  \begin{itemize}
    \item Ingen løsning, som virker i alle tilfælde
    \item $H$ ved $x_0$
    \item I kan bruges
  \end{itemize}
\end{itemize}

## DFO

\begin{itemize}
  \item Hvis $f'$ ikke er tilgængelig, men vi ønsker at optimerer $f$
  \item Nelder-Mead
  \begin{itemize}
    \item $\{x_1,x_2,\ldots, x_{n+1}\}$ sorteret, sådan at
    \item $f(x_1)\leq f(x_2) \leq \cdots \leq f(x_{n+1})$
  \end{itemize}
\end{itemize}


# Rosenbrock funktion

```{r "Rosenbrock functions"}
f <- function(x) 100*(x[2] - x[1]^2)^2 + (1 - x[1])^2
x_min_true <- c(1, 1)
x_0 <- c(-1.2,1)
# analytic derivatives
d_f <- function(x) c(2*(x[1] - 1) - 400*x[1]*(x[2] - x[1]^2), 200*(x[2] - x[1]^2))
dd_f <- function(x) rbind(c(1200*x[1]^2 - 400*x[2] + 2, -400*x[1]), c(-400*x[1], 200))
```

# Zoom

```{r "zoom"}
zoom <- function(x_k, a_lo, a_hi, c1, c2, func = f, d_f) {
  f_k <- func(x_k)
  g_k <- d_f(x_k)
  p_k <- -g_k
  
  k <- 0
  k_max <- 1000   # Maximum number of iterations.
  done <- FALSE
  
  while(!done) {
    k <- k + 1
    phi_lo <- func(x_k + a_lo*p_k)
    
    a_k <- 0.5*(a_lo + a_hi)
    phi_k <- func(x_k + a_k*p_k)
    dphi_k_0 <- g_k%*%p_k
    l_k <- f_k + c1*a_k* as.numeric(dphi_k_0)
    
    if ((phi_k > l_k) | (phi_k >= phi_lo)) {
      a_hi <- a_k
    } else {
      dphi_k <- p_k %*% d_f(x_k + a_k*p_k)
      
      if (abs(dphi_k) <= -c2*dphi_k_0) {
        return(a_k)
      }
      
      if (dphi_k*(a_hi - a_lo) >= 0) {
        a_hi <- a_lo
      }
      
      a_lo <- a_k
    }
    
    done <- (k > k_max)
  }
  
  return(a_k)
}

alpha <- function(a_0, x_k, c1, c2, func = f, d_f) {
  a_max <- 2*a_0 # Maximum step length. Can also be given as argument.
  f_k <- func(x_k)
  phi_k <- f_k
  a_1 <- a_0
  a0 <- 0
  a_k <- a_1
  a_k_old <- a0
  
  k <- 0
  k_max <- 1000   # Maximum number of iterations.
  done <- FALSE
  while(!done) {
    k <- k + 1
    f_k <- func(x_k)
    g_k <- d_f(x_k)
    p_k <- -g_k
    
    phi_k_old <- phi_k
    phi_k <- func(x_k + a_k*p_k)
    dphi_k_0 <- g_k%*%p_k
    l_k <- f_k + c1*a_k*dphi_k_0
    
    if ((phi_k > l_k) || ((k > 1) && (phi_k >= phi_k_old))) {
      return(zoom(x_k, a_k_old, a_k, c1, c2, f, d_f))
    }
    
    dphi_k <- p_k %*% d_f(x_k + a_k*p_k)
    
    if (abs(dphi_k) <= -c2*dphi_k_0) {
      return(a_k)
    }
    
    if (dphi_k >= 0) {
      return(zoom(a_k, a_k_old, x_k, c1, c2,f,d_f))
    }
    
    a_k_old <- a_k
    a_k <- rho*a_k + (1 - rho)*a_max # e.g. rho <- 0.5
    done <- (k > k_max)
  }
  
  return(a_k)
}

#params
c1 <- 0.0002
c2 <- 0.5
a_lo <- 0.1
a_hi <- 3
a_0 <- 1
n <- 2
```

# BFGS

```{r "BFGS"}
H_fun <- function(n, H, s_k, y_k) {
  I <- diag(n)
  rho_k <- as.numeric(1/(t(y_k) %*% s_k))
  (I - rho_k * s_k %*% t(y_k)) %*% H %*% (I - rho_k * y_k %*% t(s_k)) + 
    rho_k * s_k %*% t(s_k)
}

BFGS <- function(x_0, fun, dfun, zoom, stoej, output = F) {
  H_k <- diag(n)
  x_k <- x_0
  itt <- 0
  H_diff <- c()
  H_diff_ND <- c()
  while (norm(dfun(x_k),"2") > 1e-5) {
    itt <- itt + 1
    if (output == T) {
      # tjek hvor tæt estimeret H og H er
      H_diff[itt] <- norm(H_k - solve(dd_f(x_k)), "F") 
      H_diff_ND[itt] <- norm(H_k - solve(hessian(f, x_k)), "F")
    }
    p_k <- - H_k %*% dfun(x_k)
    x_k_old <- x_k
    ifelse(zoom == T, a_k <- zoom(x_k,a_lo,a_hi,c1,c2,f,d_f), a_k <- 1)
    # ifelse(zoom == T, a_k <- alpha(a_0, x_k, c1, c2, f, d_f), a_k <- 1)
    x_k <- x_k + a_k * p_k
    s_k <- x_k - x_k_old
    y_k <- dfun(x_k) - dfun(x_k_old)
    H_k <- H_fun(n, H_k, s_k, y_k)
    # Tjek for BFGS evne til at korrigere
    if (itt == 30 & stoej == T) {
      H_k <- H_k + diag(n) * 20 
    }
    if (itt == 100 & stoej == T) {
      H_k[1,2] <- H_k[1,1] + 1000
      H_k <- H_k - diag(n) * 80
    }
  }
  if (output == T) {
    loft <- H_diff < 10000
    #plot af forskel fundet ved numDeriv eller analytisk hessian
    plot(seq_along(H_diff[loft]), y = H_diff[loft])
    plot(seq_along(H_diff_ND[loft]), y = H_diff_ND[loft])
    cat("Frobenius norm forskelle stoerre end 100 =", "\n", H_diff[!loft], "\n")
    cat("x* =", x_k, "\n", "antal iterationer =", itt)
  } else
    cat("x* =", x_k, "\n", "antal iterationer =", itt)
}

BFGS(x_0, f, d_f, zoom = F, stoej = F, output = T)
BFGS(x_0, f, d_f, zoom = F, stoej = T, output = T)
BFGS(x_0, f, d_f, zoom = T, stoej = F, output = T)
```

# Nelder-Mead

```{r "Nelder-Mead"}
x_bar_fun <- function(x) {
  1/ 2 * colSums(x[-3,])
}
x_bar_t <- function(t, xn1, x_bar) {
  x_bar + t * (xn1 - x_bar)
}

f_order <- function(x,f) {
  resul <- apply(x, 1, f)
  order(resul)
}

Nelder_mead <- function(x_0, f, output = F) {
  x_n <- optimsimplex(method = "spendley", x0 = x_0)$newobj$x
  itt <- 0
  while (abs(f(x_n[3,] - f(x_n[1,]))) > 1e-10) {
    itt <- itt + 1
    x_n_old <- x_n
    order <- f_order(x_n, f)
    x_n <- x_n[order,]
    x_bar <- x_bar_fun(x_n)
    
    x_bar_m1 <- x_bar_t(-1, x_n[3,], x_bar)
    f_x_m1 <- f(x_bar_m1)
    if ( f(x_n[1,]) <= f_x_m1 & f_x_m1 < f(x_n[2,])) {
      x_n[3,] <- x_bar_m1
    }
    else 
      if (f_x_m1 < f(x_n[1,])) {
        x_bar_m2 <-x_bar_t(-2, x_n[3,], x_bar)
        f_x_m2 <- f(x_bar_m2)
        if (f_x_m2 < f_x_m1) {
          x_n[3,] <- x_bar_m2
        }
        else {
          x_n[3,] <- x_bar_m1
        }
      }
    
    else { 
      if (f_x_m1 >= f(x_n[2,])) {
        if (f(x_n[2,]) <= f_x_m1 & f_x_m1 < f(x_n[3,])) {
          x_bar_m12 <- x_bar_t(-0.5, x_n[3,], x_bar)
          f_x_m12 <- f(x_bar_m12)
          if (f_x_m12 <= f_x_m1) {
            x_n[3,] <- x_bar_m12
          }
        }
        else {
          x_bar_p12 <- x_bar_t(0.5, x_n[3,], x_bar)
          f_x_p12 <- f(x_bar_p12)
          if (f_x_p12 < f(x_n[3,])) {
            x_n[3,] <- x_bar_p12
          }
        }
      }
    }
    if (norm(x_n - x_n_old, type = "F") < 0.001) {
      for (i in 2:3) {
        x_n[i,] <- 0.5 * (x_n[1,] + x_n[i,])
      }
    }
  }
  if (output == T) {
    cat("x* = ", x_n[3,], "\n", "f(x*) = ", f(x_n), "\n", "Antal iterationer=", itt)
  }
}

Nelder_mead(x_0, f, output = T)

```

# Gradient descent, identitets matrix

```{r "Gradient descent - steepest descent - identitet"}
optimer_identitet_zoom <- function(x_0, f, d_f, output = F) {
  x_k <- x_0
  a_k <- 2
  itt <- 0
  while (norm(d_f(x_k), type = "2") > 1e-5) {
    itt <- 1 + itt
    p_k <- -d_f(x_k)
    a_k <- alpha(a_0, x_k, 0.1, 0.9,f, d_f)
    x_k <- x_k + a_k * t(p_k)
    if (itt > 100000) {
      stop("Too many iterations")
    }
  }
  if (output == T) {
    cat("x* = ", x_k, "\n", "f(x*) =", f(x_k), "\n", "iteration =", itt)
  }
}
optimer_identitet_zoom(x_0, f, d_f, output = T)
```

# Gradient descent, newton

```{r "Gradient descent - steepest descent - Newton"}
optimer_newton_zoom <- function(x_0, f, d_f, dd_f, output = F) {
  x_k <- x_0
  a_k <- 2
  itt <- 0
  while (norm(t(d_f(x_k)), type = "2") > 1e-4) {
    itt <- 1 + itt
    p_k <- solve(dd_f(x_k), -d_f(x_k))
    a_k <- alpha(a_0, x_k, 0.1,0.9, f, d_f)
    x_k <- x_k + a_k * t(p_k)
    if (itt > 100000) {
      stop("Too many iterations")
    }
  }
  if (output == T) {
    cat("x* = ", x_k, "\n", "f(x*) =", f(x_k), "\n", "iteration =", itt)
  }
}
optimer_newton_zoom(x_0, f, d_f, dd_f, output = T)
```

\newpage

# Benchmark af Rosenbrock funktion

```{r "Benchmarking Rosenbrock function"}
microbenchmark(optim(x_0, f, method = "BFGS"),
               optim(x_0, f, method = "Nelder-Mead"),
               BFGS(x_0, f, d_f, F, stoej = F), 
               Nelder_mead(x_0, f), 
               optimer_identitet_zoom(x_0, f, d_f), 
               optimer_newton_zoom(x_0, f, d_f, dd_f) , times = 2)
```

# Convex Elliptical funktion

```{r "Convex Elliptical function"}
f <- function(x) 0.5*(100*x[1]^2 + x[2]^2)
f_xy <- function(x,y) 0.5*(100*x^2 + y^2)
x_min_true <- c(0, 0)
x_0 <- c(-1.2,1.3)

# analytic derivatives
d_f <- function(x) c(0.5*200*x[1], 0.5*2*x[2])
dd_f <- function(x) 0.5*rbind(c(200, 0), c(0, 2))
```

\newpage

# Benchmarking Convex Elliptical funktion

```{r "Benchmarking Convex Elliptical function"}
microbenchmark(optim(x_0, f, method = "BFGS"),
               optim(x_0, f, method = "Nelder-Mead"),
               BFGS(x_0, f, d_f, F, stoej = F), 
               Nelder_mead(x_0, f), 
               optimer_identitet_zoom(x_0, f, d_f), 
               optimer_newton_zoom(x_0, f, d_f, dd_f) , times = 2)
```

# Nonconvex sines funktion

```{r "Nonconvex sines function" }
f <- function(x) x[1]^2 + .25*x[2]^2 + 4*(x[1] - x[2])^2*sin(x[2])^2
x_min_true <- c(0, 0)
x_0 <- c(-1,1)

# analytic derivatives
d_f <- function(x) c(2*x[1] + 8*(x[1] - x[2])*sin(x[2])^2, .5*x[2] - 8*(x[1] - x[2])*sin(x[2])^2 + 4*(x[1] - x[2])^2*sin(2*x[2]))
dd_f <- function(x) {
  H11 <- 2 + 8*sin(x[2])^2
  H12 <- 8*(2*(x[1] - x[2])*cos(x[2]) - sin(x[2]))*sin(x[2])
  H21 <- H12
  H22 <- 4.5 + 4*(2*(x[1] - x[2])^2 - 1)*cos(2*x[2]) + 16*(x[2] - x[1])*sin(2*x[2])
  rbind(c(H11, H12), c(H21, H22))
}
```

\newpage

# Benchmarking Nonconvex sines funktion

```{r "Benchmarking"}
microbenchmark(optim(x_0, f, method = "BFGS"),
               optim(x_0, f, method = "Nelder-Mead"),
               BFGS(x_0, f, d_f, F, stoej = F), 
               Nelder_mead(x_0, f), 
               optimer_newton_zoom(x_0, f, d_f, dd_f) , times = 2)
```
